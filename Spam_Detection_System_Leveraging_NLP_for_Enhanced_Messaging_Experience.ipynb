{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##  **Spam Detection System: Leveraging NLP for Enhanced Messaging Experience**\n",
        "\n",
        "As a passionate individual with a strong interest in data science and Natural Language Processing (NLP), I embarked on an exciting project to develop a robust spam detection system. This project showcases my ability to harness cutting-edge NLP techniques and machine learning algorithms to address real-world challenges.\n",
        "\n",
        "## Project Overview:\n",
        "\n",
        "The aim of this project was to design an efficient spam detection system for mobile messaging. I leveraged the SMS Spam Collection Dataset from Kaggle and employed Python along with popular libraries like NLTK and scikit-learn. The ultimate goal was to create a model capable of accurately classifying incoming messages as either spam or ham (non-spam) in real-time.\n",
        "\n",
        "## Key Highlights:\n",
        "\n",
        "1. Data Preprocessing: I meticulously cleaned and processed the raw text data by converting it to lowercase, tokenizing, removing stopwords, and employing stemming. This critical step ensured that the model could effectively analyze and understand the messages.\n",
        "\n",
        "2. Feature Extraction: Employing the powerful TF-IDF vectorization technique, I transformed the preprocessed text messages into numerical features, making them suitable for machine learning algorithms.\n",
        "\n",
        "3. Machine Learning Model: I utilized the Naive Bayes classifier for its efficiency in handling text data. The model was trained on the processed data to predict whether a given message is spam or ham.\n",
        "\n",
        "4. Model Evaluation: The trained model achieved an impressive accuracy of 96% in classifying messages, reflecting its effectiveness in distinguishing between spam and legitimate content. Additionally, I analyzed the confusion matrix and classification report to gain valuable insights into the model's performance, including precision and recall metrics.\n",
        "\n",
        "## Conclusion:\n",
        "\n",
        "This project serves as a testament to my ability as an individual data scientist, showcasing my proficiency in NLP techniques, data preprocessing, and machine learning model development. By building this spam detection system, I have demonstrated my capability to create impactful solutions that can significantly improve customer experience and satisfaction by reducing the impact of spam messages.\n",
        "\n",
        "I am excited to present this project as a representation of my skills and passion for data science. Leveraging NLP for spam detection, I aim to contribute my expertise to various industries, solving complex challenges and delivering valuable insights that drive positive outcomes.\n",
        "\n"
      ],
      "metadata": {
        "id": "6p-ef0w_pbgP"
      },
      "id": "6p-ef0w_pbgP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contents:\n",
        "\n",
        "1. Business probleme.    \n",
        "2. Loading libraries.      \n",
        "3. Loading the Dataset.           \n",
        "4. Data Preprocessing and Featuer Engineering.   \n",
        "  4.1. Remove unnecessary columns.               \n",
        "  4.2. Convert labels to binary (0 for 'ham', 1 for 'spam')           \n",
        "5. Text Preprocessing.\n",
        "6. Split the Data into Training and Testing Sets.\n",
        "7. Feature Extraction.\n",
        "8. Train the Model (Naive Bayes Classifier).\n",
        "9. Make Predictions.\n",
        "10. Evaluate the Model and analysis the results.\n",
        "11. Conclusions."
      ],
      "metadata": {
        "id": "z3LFnXbxrs0w"
      },
      "id": "z3LFnXbxrs0w"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1- Business probleme\n",
        "the company has noticed a significant increase in customer complaints related to spam messages. These spam messages not only annoy customers but also create a negative impact on the overall customer experience and satisfaction.\n",
        "\n",
        "The business problem is to develop an efficient spam detection system that can accurately identify spam messages and prevent them from reaching customers' inboxes. The company aims to reduce the number of spam messages received by customers, which will lead to improved customer satisfaction and loyalty.\n",
        "\n",
        "To address this problem, the data scientist's task is to build a robust machine learning model using Natural Language Processing (NLP) techniques to classify incoming messages as either \"spam\" or \"ham\" (non-spam). The model should be able to process and analyze large volumes of messages in real-time to effectively filter out spam messages before they reach customers' devices.\n",
        "\n",
        "The solution should integrate seamlessly with the existing messaging infrastructure of the mobile service provider, ensuring minimal disruption to the customers' messaging experience. By implementing an accurate and efficient spam detection system, the company aims to enhance customer satisfaction, reduce customer complaints, and strengthen its position in the highly competitive mobile service market.\n",
        "Data Set, which is available at https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset."
      ],
      "metadata": {
        "id": "JohfFIrEoe4h"
      },
      "id": "JohfFIrEoe4h"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2- Loading libraries"
      ],
      "metadata": {
        "id": "fMGqcW78SwhV"
      },
      "id": "fMGqcW78SwhV"
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "infrared-copper",
      "metadata": {
        "id": "infrared-copper"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "misQMxMi8xO2",
        "outputId": "004854a6-4f5b-4dd0-866b-016f97c31871"
      },
      "id": "misQMxMi8xO2",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3- Loading the Dataset"
      ],
      "metadata": {
        "id": "V4QyWeAiTPcY"
      },
      "id": "V4QyWeAiTPcY"
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('spam.csv', encoding='ISO-8859-1')\n"
      ],
      "metadata": {
        "id": "bAjcl62peiVo"
      },
      "id": "bAjcl62peiVo",
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4- Data Preprocessing\n",
        "# 4.1- Remove unnecessary columns"
      ],
      "metadata": {
        "id": "SVwlYsa0AKE-"
      },
      "id": "SVwlYsa0AKE-"
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[['v1', 'v2']]\n",
        "data.columns = ['label', 'text']"
      ],
      "metadata": {
        "id": "3ZxdLy-24AEU"
      },
      "id": "3ZxdLy-24AEU",
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.2- Convert labels to binary (0 for 'ham', 1 for 'spam')"
      ],
      "metadata": {
        "id": "J8Ipdh1NAcyc"
      },
      "id": "J8Ipdh1NAcyc"
    },
    {
      "cell_type": "code",
      "source": [
        "data['label'] = data['label'].map({'ham': 0, 'spam': 1})\n"
      ],
      "metadata": {
        "id": "DTdHysvA4ITb"
      },
      "id": "DTdHysvA4ITb",
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5- Text Preprocessing"
      ],
      "metadata": {
        "id": "v9CGRx57Ajhl"
      },
      "id": "v9CGRx57Ajhl"
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    words = nltk.word_tokenize(text.lower())  # Convert to lowercase and tokenize\n",
        "    words = [word for word in words if word.isalpha()]  # Remove non-alphabetic characters\n",
        "    words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
        "    words = [stemmer.stem(word) for word in words]  # Stemming\n",
        "    return ' '.join(words)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVxCEsjo4-sp",
        "outputId": "b53183ff-a2b3-4fc2-ea57-6874ea9479d7"
      },
      "id": "gVxCEsjo4-sp",
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    words = nltk.word_tokenize(text.lower())  # Convert to lowercase and tokenize\n",
        "    words = [word for word in words if word.isalpha()]  # Remove non-alphabetic characters\n",
        "    words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
        "    words = [stemmer.stem(word) for word in words]  # Stemming\n",
        "    return ' '.join(words)\n",
        "\n"
      ],
      "metadata": {
        "id": "6yPoItvw5BPQ"
      },
      "id": "6yPoItvw5BPQ",
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['text'] = data['text'].apply(preprocess_text)\n"
      ],
      "metadata": {
        "id": "qOntBJKt5LsJ"
      },
      "id": "qOntBJKt5LsJ",
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6- Split the Data into Training and Testing Sets"
      ],
      "metadata": {
        "id": "qpx2WZurBocP"
      },
      "id": "qpx2WZurBocP"
    },
    {
      "cell_type": "code",
      "source": [
        "X = data['text']\n",
        "y = data['label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "gQLAktXz9BcU"
      },
      "id": "gQLAktXz9BcU",
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7-  Feature Extraction"
      ],
      "metadata": {
        "id": "bkJTSgXWBxga"
      },
      "id": "bkJTSgXWBxga"
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
      ],
      "metadata": {
        "id": "u8_zOXom9EyG"
      },
      "id": "u8_zOXom9EyG",
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8- Train the Model (Naive Bayes Classifier)"
      ],
      "metadata": {
        "id": "Jxsbu2LPB60b"
      },
      "id": "Jxsbu2LPB60b"
    },
    {
      "cell_type": "code",
      "source": [
        "naive_bayes_classifier = MultinomialNB()\n",
        "naive_bayes_classifier.fit(X_train_tfidf, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "l-xBj4vY9JO_",
        "outputId": "7403bd78-bf42-4737-8f0f-00952a0e54d4"
      },
      "id": "l-xBj4vY9JO_",
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9- Make Predictions"
      ],
      "metadata": {
        "id": "kZNpV5o_B_b_"
      },
      "id": "kZNpV5o_B_b_"
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = naive_bayes_classifier.predict(X_test_tfidf)"
      ],
      "metadata": {
        "id": "f5uYlIJE9NxY"
      },
      "id": "f5uYlIJE9NxY",
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10- Evaluate the Model"
      ],
      "metadata": {
        "id": "JluoQVgqCJJk"
      },
      "id": "JluoQVgqCJJk"
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)\n",
        "\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_rep)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhzHllNQ9SNo",
        "outputId": "992b701c-cc81-4582-bd44-00ca68449e99"
      },
      "id": "UhzHllNQ9SNo",
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.96\n",
            "Confusion Matrix:\n",
            "[[965   0]\n",
            " [ 42 108]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      1.00      0.98       965\n",
            "           1       1.00      0.72      0.84       150\n",
            "\n",
            "    accuracy                           0.96      1115\n",
            "   macro avg       0.98      0.86      0.91      1115\n",
            "weighted avg       0.96      0.96      0.96      1115\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## analysis of the results for Evaluate the Model:\n",
        "\n",
        "1. The model achieved an impressive accuracy of 96%, indicating that it correctly classified the majority of messages as either spam or ham.\n",
        "2. The confusion matrix shows that out of 965 ham messages, all were correctly classified as ham (True Positives). Also, out of 150 spam messages, 108 were correctly classified as spam (True Positives), while 42 were incorrectly classified as ham (False Negatives).\n",
        "3. The model's precision for spam (class 1) is 100%, indicating that when it predicts a message as spam, it is highly likely to be correct. However, its recall for spam is 72%, implying that it may not capture all spam messages, leading to some false negatives.\n",
        "4. The weighted average F1-score of 0.96 is a balanced metric that considers both precision and recall, indicating a well-performing model overall."
      ],
      "metadata": {
        "id": "n9B0BzprGVtj"
      },
      "id": "n9B0BzprGVtj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11- Conclusions:\n",
        "\n",
        "1. The spam detection model demonstrates high accuracy and precision, effectively identifying the majority of spam messages.\n",
        "2. The model's recall for spam can be further improved to capture more spam messages and reduce the number of false negatives.\n",
        "3. Continuous monitoring and updates to the model are essential to adapt to new spam patterns and maintain its effectiveness over time.\n",
        "\n",
        "\n",
        "Overall, the spam detection system appears promising and can be integrated into the mobile service provider's messaging infrastructure to enhance customer experience by reducing the impact of spam messages and improving overall customer satisfaction. However, ongoing optimization and evaluation of the model are necessary to ensure its efficiency in combating new and evolving spam tactics."
      ],
      "metadata": {
        "id": "-L842ljvGzfR"
      },
      "id": "-L842ljvGzfR"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}